{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-o2j6w9Qdw7B"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_naive(X: torch.Tensor, W_q, W_k, W_v,\n",
        "                    b_q, b_k, b_v, device) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "        X :     NxD (N: number of tokens, D: Dimension of latent space tokenizer)\n",
        "        W_*:    D_headxD (D_head: Model space dimension / num heads)\n",
        "    \"\"\"\n",
        "\n",
        "    # check if X is NxD\n",
        "    assert(X.dim()==2)\n",
        "\n",
        "    Q = torch.matmul(X, W_q.transpose(0,1)) + b_q[None, :]\n",
        "    K = torch.matmul(X, W_k.transpose(0,1)) + b_k[None, :]\n",
        "    V = torch.matmul(X, W_v.transpose(0,1)) + b_v[None, :]\n",
        "    D_V = torch.tensor(V.shape[1], device=device)\n",
        "\n",
        "    KQ_normalised = torch.matmul(Q, K.transpose(0,1)) / torch.sqrt(D_V)\n",
        "    KQ_softmax = torch.softmax(KQ_normalised, dim=1)\n",
        "\n",
        "    attention = torch.matmul(KQ_softmax, V)\n",
        "\n",
        "    return attention\n",
        "\n",
        "def multiheaded_attention_naive(X: torch.Tensor, W_qkv, W_out,\n",
        "                    b_qkv, b_out, num_heads=1, device=\"cuda\") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    W_qkv: 3DxD\n",
        "    W_out: DxD\n",
        "    b_qkv: 3D\n",
        "    b_out: D\n",
        "    \"\"\"\n",
        "    # check if X is NxD\n",
        "    assert(X.dim()==2)\n",
        "\n",
        "    N, D = X.shape\n",
        "    D_head = math.ceil(D / num_heads)\n",
        "    attention = torch.empty((N, D), device=device, dtype=torch.float16)\n",
        "\n",
        "    for head in range(num_heads):\n",
        "        head_start = head*D_head\n",
        "        head_end = min(D, (head+1)*D_head)\n",
        "        attention[:,head_start:head_end] = attention_naive(\n",
        "            X,\n",
        "            W_qkv[0:D, :][head_start:head_end, :],\n",
        "            W_qkv[D:2*D, :][head_start:head_end, :],\n",
        "            W_qkv[2*D:3*D, :][head_start:head_end, :],\n",
        "            b_qkv[0+head_start:0+head_end],\n",
        "            b_qkv[D+head_start:D+head_end],\n",
        "            b_qkv[2*D+head_start:2*D+head_end],\n",
        "            device\n",
        "        )\n",
        "\n",
        "    attention = torch.matmul(attention, W_out.transpose(0,1)) + b_out[None, :]\n",
        "\n",
        "    return attention"
      ],
      "metadata": {
        "id": "8At_PzfpeV0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _attention(\n",
        "        Q, K, V,\n",
        "        O, l, m,\n",
        "        BLOCK_R : tl.constexpr,\n",
        "        BLOCK_C : tl.constexpr,\n",
        "        BLOCK_D : tl.constexpr,\n",
        "        N_q : tl.constexpr, N_v : tl.constexpr,\n",
        "        D_head : tl.constexpr, B_c : tl.constexpr, B_r : tl.constexpr,\n",
        "        T_r : tl.constexpr, T_c : tl.constexpr,\n",
        "        ):\n",
        "\n",
        "    head_idx = tl.program_id(0)\n",
        "    # transpose everything when passing args, so that moving across is linear.\n",
        "    Q_ptr = Q + (head_idx * N_q * D_head)\n",
        "    K_ptr = K + (head_idx * N_v * D_head)\n",
        "    V_ptr = V + (head_idx * N_v * D_head)\n",
        "    O_ptr = O + (head_idx * N_q * D_head)\n",
        "    l_ptr = l + (head_idx * N_q)\n",
        "    m_ptr = m + (head_idx * N_q)\n",
        "\n",
        "    # Get ptrs for loading K,V,Q,O into SRAM\n",
        "    D_indices = tl.arange(0, BLOCK_D)[:, None]          # (D_head, 1)\n",
        "    C_indices = tl.arange(0, BLOCK_C)[None, :]          # (1, B_c)\n",
        "    R_indices = tl.arange(0, BLOCK_R)[None, :]          # (1, B_r)\n",
        "    # Moving the start of each row by N_v steps\n",
        "    # then get B_c size slices from each row\n",
        "    K_row_start_ptrs = K_ptr + (D_indices * N_v)            # (D_head, 1)\n",
        "    K_j_ptrs = K_row_start_ptrs + C_indices                 # (D_head, B_c)\n",
        "    V_row_start_ptrs = V_ptr + (D_indices * N_v)            # (D_head, 1)\n",
        "    V_j_ptrs = V_row_start_ptrs + C_indices                 # (D_head, B_c)\n",
        "    mask_KV = (D_indices < D_head) & (C_indices < B_c)      # (BLOCK_D, BLOCK_C)\n",
        "\n",
        "    # Same for Q and O\n",
        "    Q_row_start_ptrs = Q_ptr + (D_indices * N_q)            # (D_head, 1)\n",
        "    Q_i_ptrs = Q_row_start_ptrs + R_indices                 # (D_head, B_r)\n",
        "    O_row_start_ptrs = O_ptr + (D_indices * N_q)            # (D_head, 1)\n",
        "    O_i_ptrs = O_row_start_ptrs + R_indices                 # (D_head, B_r)\n",
        "    mask_QO = (D_indices < D_head) & (R_indices < B_r)      # (BLOCK_D, BLOCK_R)\n",
        "\n",
        "    # Get the pointers for l and m\n",
        "    l_i_ptrs = l_ptr + R_indices                            # (BLOCK_R,)\n",
        "    m_i_ptrs = m_ptr + R_indices                            # (BLOCK_R,)\n",
        "    mask_lm = R_indices < B_r                               # (BLOCK_R,)\n",
        "\n",
        "    for j in range(T_c):\n",
        "        K_j = tl.load(K_j_ptrs, mask=mask_KV, other=0.0)    # (D_head, B_c)\n",
        "        V_j = tl.load(V_j_ptrs, mask=mask_KV, other=0.0)    # (D_head, B_c)\n",
        "\n",
        "        for i in range(T_r):\n",
        "            Q_i = tl.load(Q_i_ptrs, mask=mask_QO, other=0.0)    # (D_head, B_r)\n",
        "            O_i = tl.load(O_i_ptrs, mask=mask_QO, other=0.0)    # (D_head, B_r)\n",
        "            l_i = tl.load(l_i_ptrs, mask=mask_lm, other=0.0)\n",
        "            m_i = tl.load(m_i_ptrs, mask=mask_lm, other=float('-inf'))\n",
        "\n",
        "            # Compute S_ij = Q_i x K_j^T\n",
        "            scale = 1.0 / tl.sqrt(tl.full([], D_head, dtype=tl.float32))  # scalar\n",
        "            S_ij = tl.dot(K_j.T, Q_i) * scale               # (B_c, B_r)\n",
        "\n",
        "            # get the rowmax for S_ij\n",
        "            m_ij = tl.max(S_ij, axis=0)                     # (B_r,)\n",
        "\n",
        "            # compute exponents for softmax after subtracting max\n",
        "            # element for numerical stability\n",
        "            P_ij = tl.exp(S_ij - m_ij)                      # (B_c, B_r)\n",
        "\n",
        "            # Compute the row sums for softmax\n",
        "            l_ij = tl.sum(P_ij, axis=0)                     # (B_r,)\n",
        "\n",
        "            # Update the global max m_i till this point\n",
        "            # tl.cat threw an error when given dim=0, weird but okay because it defaults to 0\n",
        "            # m_i_new = tl.max(tl.cat(m_ij, m_i[None, :]), axis=0, keep_dims=True)     # (1, B_r)\n",
        "            m_i_new = tl.maximum(m_ij, m_i)                 # (B_r,)\n",
        "            alpha = tl.exp(m_i - m_i_new)                   # (B_r,)\n",
        "            beta  = tl.exp(m_ij - m_i_new)                  # (B_r,)\n",
        "            l_i_new = alpha * l_i + beta * l_ij               # (B_r,)\n",
        "            # l_i_new = (tl.exp(m_i[None, :] - m_i_new) * l_i[None, :] +\n",
        "            #            tl.exp(m_ij - m_i_new) * l_ij)       # (1, B_r)\n",
        "\n",
        "            # Update the output\n",
        "            # create a diagonal matrix with l_i on diag\n",
        "            # mask_primitive = tl.arange(0, l_i.shape[0])\n",
        "            # diag_l_i = l_i[None, :] * (mask_primitive[:, None] == mask_primitive[None, :])\n",
        "            # diag_l_i_new_inv = (1 / l_i_new) * (mask_primitive[:, None] == mask_primitive[None, :])\n",
        "\n",
        "            # Broadcasting works here as the last dim is matched (D_head, Br) and (Br)\n",
        "            PV = tl.dot(V_j, P_ij)                          # (D_head, B_r)\n",
        "            O_i = ((l_i * alpha) * O_i + beta * PV) / l_i_new       # (D_head, B_r)\n",
        "\n",
        "            # Store the computed O_i, l_i and m_i values\n",
        "            tl.store(O_i_ptrs, O_i, mask=mask_QO)\n",
        "            tl.store(l_i_ptrs, l_i_new, mask=mask_lm)\n",
        "            tl.store(m_i_ptrs, m_i_new, mask=mask_lm)\n",
        "\n",
        "            Q_i_ptrs += B_r\n",
        "            O_i_ptrs += B_r\n",
        "            l_i_ptrs += B_r\n",
        "            m_i_ptrs += B_r\n",
        "\n",
        "        K_j_ptrs += B_c    # shift ptrs for next iter\n",
        "        V_j_ptrs += B_c\n",
        "        Q_i_ptrs -= N_q\n",
        "        O_i_ptrs -= N_q\n",
        "        l_i_ptrs -= N_q\n",
        "        m_i_ptrs -= N_q\n",
        "\n",
        "\n",
        "def multiheaded_attention_triton(\n",
        "        query: torch.Tensor,\n",
        "        key: torch.Tensor,\n",
        "        value: torch.Tensor,\n",
        "        W_qkv, W_out,\n",
        "        b_qkv, b_out,\n",
        "        num_heads=1,\n",
        "        device=\"cuda\") -> torch.Tensor:\n",
        "\n",
        "    N_q, D = query.shape\n",
        "    N_v, _ = value.shape\n",
        "    dtype = W_qkv.dtype\n",
        "\n",
        "    M = 48*1024     # HARD CODED SRAM\n",
        "\n",
        "    # Q = torch.matmul(query, W_qkv[0:D, :].transpose(0,1)) + b_qkv[0:D][None, :]\n",
        "    # K = torch.matmul(query, W_qkv[D:2*D, :].transpose(0,1)) + b_qkv[D:2*D][None, :]\n",
        "    # V = torch.matmul(query, W_qkv[2*D:3*D, :].transpose(0,1)) + b_qkv[2*D:3*D][None, :]\n",
        "\n",
        "    # buffers\n",
        "    Q = torch.zeros((N_q, D), device=device, dtype=dtype)\n",
        "    K = torch.zeros((N_v, D), device=device, dtype=dtype)\n",
        "    V = torch.zeros((N_v, D), device=device, dtype=dtype)\n",
        "    D_head = math.ceil(D / num_heads)\n",
        "\n",
        "    for head in range(num_heads):\n",
        "        hs = head * D_head\n",
        "        he = min(D, (head + 1) * D_head)\n",
        "\n",
        "        Wq = W_qkv[0:D, :][hs:he, :]\n",
        "        Wk = W_qkv[D:2*D, :][hs:he, :]\n",
        "        Wv = W_qkv[2*D:3*D, :][hs:he, :]\n",
        "\n",
        "        bq = b_qkv[hs:he]\n",
        "        bk = b_qkv[D + hs:D + he]\n",
        "        bv = b_qkv[2*D + hs:2*D + he]\n",
        "\n",
        "        Q[:, hs:he] = (query @ Wq.T) + bq[None, :]\n",
        "        K[:, hs:he] = (key   @ Wk.T) + bk[None, :]\n",
        "        V[:, hs:he] = (value @ Wv.T) + bv[None, :]\n",
        "\n",
        "    # multiheads are cascaded column wise, by taking the transpose, we can access each head in continuous space\n",
        "    Q = Q.T\n",
        "    K = K.T\n",
        "    V = V.T\n",
        "\n",
        "    B_c = math.ceil(M / (4*D))\n",
        "    B_r = min(math.ceil(M / (4*D)), D)\n",
        "\n",
        "    O = torch.zeros((D, N_q), device=device, dtype=dtype)    # transposed to match Q,K,V\n",
        "\n",
        "    l = torch.zeros((num_heads, N_q), device=device, dtype=dtype)\n",
        "    m = torch.full((num_heads, N_q), float('-inf'), device=device, dtype=dtype)\n",
        "\n",
        "    T_r = math.ceil(N_q / B_r)\n",
        "    T_c = math.ceil(N_v / B_c)\n",
        "\n",
        "    BLOCK_R = triton.next_power_of_2(B_r)\n",
        "    BLOCK_C = triton.next_power_of_2(B_c)\n",
        "    BLOCK_D = triton.next_power_of_2(D_head)\n",
        "\n",
        "    num_warps = 16\n",
        "    grid = (num_heads, )\n",
        "\n",
        "    _attention[grid](\n",
        "        Q, K, V,\n",
        "        O, l, m,\n",
        "        BLOCK_R, BLOCK_C, BLOCK_D,\n",
        "        N_q, N_v,\n",
        "        D_head, B_c, B_r,\n",
        "        T_r, T_c,\n",
        "        num_warps=num_warps\n",
        "    )\n",
        "\n",
        "    print(O.dtype, W_out.dtype)\n",
        "\n",
        "    attention = torch.matmul(O.T, W_out.transpose(0,1)) + b_out[None, :]\n",
        "\n",
        "    return attention"
      ],
      "metadata": {
        "id": "pPaq8E4GZYHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Timing helpers (your style)\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def time_ms0(fn, iters=100, warmup=25):\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end   = torch.cuda.Event(enable_timing=True)\n",
        "    start.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return start.elapsed_time(end) / iters\n",
        "\n",
        "def mha_naive_wrapper(X, mha: torch.nn.MultiheadAttention):\n",
        "    num_heads = mha.num_heads\n",
        "    in_proj_w = mha.in_proj_weight\n",
        "    in_proj_b = mha.in_proj_bias\n",
        "    out_proj = mha.out_proj\n",
        "    device = in_proj_w.device\n",
        "\n",
        "    attention_naive_out = multiheaded_attention_naive(\n",
        "        X,\n",
        "        in_proj_w,\n",
        "        out_proj.weight,\n",
        "        in_proj_b,\n",
        "        out_proj.bias,\n",
        "        num_heads,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    return attention_naive_out\n",
        "\n",
        "def mha_triton_wrapper(X, mha: torch.nn.MultiheadAttention):\n",
        "    num_heads = mha.num_heads\n",
        "    in_proj_w = mha.in_proj_weight\n",
        "    in_proj_b = mha.in_proj_bias\n",
        "    out_proj = mha.out_proj\n",
        "    device = in_proj_w.device\n",
        "\n",
        "    attention_triton_out = multiheaded_attention_triton(\n",
        "        X, X, X,\n",
        "        in_proj_w,\n",
        "        out_proj.weight,\n",
        "        in_proj_b,\n",
        "        out_proj.bias,\n",
        "        num_heads,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    return attention_triton_out\n",
        "\n",
        "def mha_torch_wrapper(X, mha: torch.nn.MultiheadAttention):\n",
        "    # For (N, D) input, PyTorch interprets as (L, E) when unbatched.\n",
        "    # This returns (L, E). Use need_weights=False to time only output.\n",
        "    out, _ = mha(X, X, X, need_weights=False)\n",
        "    return out\n",
        "\n",
        "def report(name, ms, N, D):\n",
        "    toks_per_s = N / (ms / 1e3)\n",
        "    print(f\"{name:>14}: {ms:8.3f} ms | {toks_per_s:10.1f} tokens/s\")\n"
      ],
      "metadata": {
        "id": "xtmfHOIGeVt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Run benchmark\n",
        "# -----------------------------\n",
        "torch.manual_seed(42)\n",
        "device=\"cuda\"\n",
        "N, D, H = 8192, 128, 2\n",
        "\n",
        "X = torch.randn((N, D), device=device, dtype=torch.float16)\n",
        "\n",
        "mha = torch.nn.MultiheadAttention(embed_dim=D, num_heads=H, device=device, dtype=torch.float16)\n",
        "mha.eval()\n",
        "\n",
        "# correctness check first\n",
        "with torch.no_grad():\n",
        "    ref = mha_torch_wrapper(X, mha)\n",
        "    out = mha_naive_wrapper(X, mha)\n",
        "    out_triton = mha_triton_wrapper(X, mha)\n",
        "    print(\"max abs err:\", (out - ref).abs().max().item())\n",
        "    print(\"mean abs err:\", (out - ref).abs().mean().item())\n",
        "    print(\"triton max abs err:\", (out_triton - ref).abs().max().item())\n",
        "    print(\"triton mean abs err:\", (out_triton - ref).abs().mean().item())\n",
        "\n",
        "# timing\n",
        "torch_ms = time_ms0(lambda: mha_torch_wrapper(X, mha), iters=100, warmup=25)\n",
        "naive_ms = time_ms0(lambda: mha_naive_wrapper(X, mha), iters=20, warmup=5)  # naive is O(N^2); use fewer iters\n",
        "triton_ms = time_ms0(lambda: mha_triton_wrapper(X, mha), iters=100, warmup=25)\n",
        "\n",
        "report(\"torch_mha\", torch_ms, N, D)\n",
        "report(\"naive_mha\", naive_ms, N, D)\n",
        "report(\"triton_mha\", triton_ms, N, D)"
      ],
      "metadata": {
        "id": "5STli3JOeVrr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "outputId": "9b5a650d-cf7a-4011-8fe6-1fd1991c907c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "CompilationError",
          "evalue": "at 56:26:\n    for j in range(T_c):\n        K_j = tl.load(K_j_ptrs, mask=mask_KV, other=0.0)    # (D_head, B_c)\n        V_j = tl.load(V_j_ptrs, mask=mask_KV, other=0.0)    # (D_head, B_c)\n\n        for i in range(T_r):\n            Q_i = tl.load(Q_i_ptrs, mask=mask_QO, other=0.0)    # (D_head, B_r)\n            O_i = tl.load(O_i_ptrs, mask=mask_QO, other=0.0)    # (D_head, B_r)\n            l_i = tl.load(l_i_ptrs, mask=mask_lm, other=0.0)\n            m_i = tl.load(m_i_ptrs, mask=mask_lm, other=float('-inf'))\n\n            # Compute S_ij = Q_i x K_j^T\n            scale = 1.0 / tl.sqrt(tl.full([], D_head, dtype=tl.float16))  # scalar\n                          ^\nExpected dtype ['fp32', 'fp64'] but got fp16",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/language/core.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m                              \"(`_semantic` argument must be provided outside of JIT functions.)\")\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/language/math.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected dtype {dtypes} but got {arg.type.scalar.name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected dtype ['fp32', 'fp64'] but got fp16",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mCompilationError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2488549813.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmha_torch_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmha_naive_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mout_triton\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmha_triton_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max abs err:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean abs err:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2106141889.py\u001b[0m in \u001b[0;36mmha_triton_wrapper\u001b[0;34m(X, mha)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_proj_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     attention_triton_out = multiheaded_attention_triton(\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0min_proj_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3972693103.py\u001b[0m in \u001b[0;36mmultiheaded_attention_triton\u001b[0;34m(query, key, value, W_qkv, W_out, b_qkv, b_out, num_heads, device)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     _attention[grid](\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mmemorizes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \"\"\"\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0;31m# return cast(T, functools.partial(cast(Callable, self.run), grid=grid))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m                                                                     options)\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m             \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstexprs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkernel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py\u001b[0m in \u001b[0;36m_do_compile\u001b[0;34m(self, key, signature, device, constexprs, options, attrs, warmup)\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masync_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync_compile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m             \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m             \u001b[0mkernel_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             self._call_hook(knobs.runtime.jit_post_compile_hook, key, signature, device, constexprs, options, [attrs],\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(src, target, options, _env_vars)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0mmodule_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_module_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodegen_fns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mfilter_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py\u001b[0m in \u001b[0;36mmake_ir\u001b[0;34m(self, target, options, codegen_fns, module_map, context)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGPUTarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodegen_fns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcode_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mast_to_ttir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\n\u001b[0m\u001b[1;32m     81\u001b[0m                            module_map=module_map)\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCompilationError\u001b[0m: at 56:26:\n    for j in range(T_c):\n        K_j = tl.load(K_j_ptrs, mask=mask_KV, other=0.0)    # (D_head, B_c)\n        V_j = tl.load(V_j_ptrs, mask=mask_KV, other=0.0)    # (D_head, B_c)\n\n        for i in range(T_r):\n            Q_i = tl.load(Q_i_ptrs, mask=mask_QO, other=0.0)    # (D_head, B_r)\n            O_i = tl.load(O_i_ptrs, mask=mask_QO, other=0.0)    # (D_head, B_r)\n            l_i = tl.load(l_i_ptrs, mask=mask_lm, other=0.0)\n            m_i = tl.load(m_i_ptrs, mask=mask_lm, other=float('-inf'))\n\n            # Compute S_ij = Q_i x K_j^T\n            scale = 1.0 / tl.sqrt(tl.full([], D_head, dtype=tl.float16))  # scalar\n                          ^\nExpected dtype ['fp32', 'fp64'] but got fp16"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.full((5,), float('inf'), device=device)"
      ],
      "metadata": {
        "id": "U2nFVjYm6yEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "id": "g_unOZXJzl-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "925d3ce2-5253-4df5-d12c-97262ce0f358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([inf, inf, inf, inf, inf], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.zeros((1,5))\n",
        "x.shape, x[0,:].shape"
      ],
      "metadata": {
        "id": "5CA5TFbBi6kC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "627e1ced-6da9-47d0-fe55-6d2ce2c76301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5]), torch.Size([5]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What FlashAttention improves (why it’s faster + more memory-friendly)\n",
        "\n",
        "Vanilla attention does this (per head):\n",
        "\n",
        "\\[\n",
        "O = \\text{softmax}(QK^T)\\,V\n",
        "\\]\n",
        "\n",
        "The expensive part isn’t the math—it’s **memory traffic**:\n",
        "\n",
        "- Computing \\(S = QK^T\\) makes an \\(N\\times N\\) score matrix.\n",
        "- Then softmax makes another \\(N\\times N\\) matrix \\(P\\).\n",
        "- Writing/reading those huge matrices to **HBM (GPU high-bandwidth memory)** dominates runtime and blows up memory.\n",
        "\n",
        "**FlashAttention’s key idea**: never materialize \\(S\\) or \\(P\\) in HBM.  \n",
        "Instead it:\n",
        "\n",
        "- **tiles** \\(Q\\) and \\(K,V\\) into blocks that fit in fast **on-chip SRAM** (shared memory/registers),\n",
        "- computes softmax **incrementally** using an **online, numerically stable** update (running max + running normalizer),\n",
        "- **fuses** operations so the main loop is “load a tile → compute → accumulate output” with minimal writes.\n",
        "\n",
        "**Benefits you get:**\n",
        "\n",
        "- **Much lower memory usage** (no \\(N^2\\) intermediates stored).\n",
        "- **Much higher speed** by reducing HBM reads/writes and increasing arithmetic intensity.\n",
        "- Still computes the **exact same attention** as standard (up to floating point rounding), not an approximation.\n",
        "- The online softmax update is **numerically stable** (like standard log-sum-exp stabilization).\n",
        "\n",
        "---\n",
        "\n",
        "## FlashAttention Algorithm 1 — line by line\n",
        "\n",
        "I’ll keep a running dictionary of symbols:\n",
        "\n",
        "- \\(Q,K,V \\in \\mathbb{R}^{N\\times d}\\): query/key/value (for a single head typically).\n",
        "- HBM = GPU global memory, big but slower.\n",
        "- SRAM = on-chip memory (shared/register), small but fast.\n",
        "- \\(M\\): SRAM capacity budget for the kernel.\n",
        "- \\(O \\in \\mathbb{R}^{N\\times d}\\): the attention output we want.\n",
        "- For softmax stability per row, we track:\n",
        "  - \\(m\\in\\mathbb{R}^N\\): running **row max** of scores seen so far\n",
        "  - \\(\\ell\\in\\mathbb{R}^N\\): running **row sum of \\(\\exp(\\text{scores} - \\max)\\)** (the softmax denominator)\n",
        "\n",
        "---\n",
        "\n",
        "### **Require:** Matrices \\(Q,K,V\\in\\mathbb{R}^{N\\times d}\\) in HBM, on-chip SRAM of size \\(M\\)\n",
        "\n",
        "You start with Q/K/V stored in global memory. You have a limited SRAM scratchpad to hold small tiles.\n",
        "\n",
        "---\n",
        "\n",
        "### **1: Set block sizes** \\(B_c = \\left\\lfloor\\frac{M}{4d}\\right\\rfloor\\), \\(B_r=\\min\\left(\\left\\lfloor\\frac{M}{4d}\\right\\rfloor,\\, d\\right)\\)\n",
        "\n",
        "Pick tile sizes so the working set fits in SRAM.\n",
        "\n",
        "- \\(B_c\\): how many **keys/values** rows you load at once (a “column tile”).\n",
        "- \\(B_r\\): how many **queries** rows you process at once (a “row tile”).\n",
        "\n",
        "The \\(\\frac{M}{4d}\\) heuristic comes from “SRAM must hold several \\((\\text{tile rows})\\times d\\) arrays at once” (e.g., Q tile, K tile, V tile, plus intermediates). The constant “4” is a budgeting factor.\n",
        "\n",
        "---\n",
        "\n",
        "### **2: Initialize** \\(O=(0)_{N\\times d}\\), \\(\\ell=(0)_N\\), \\(m=(-\\infty)_N\\) in HBM\n",
        "\n",
        "For each query row:\n",
        "\n",
        "- output starts at 0,\n",
        "- running denominator \\(\\ell\\) starts at 0 (no probability mass accumulated yet),\n",
        "- running max \\(m\\) starts at \\(-\\infty\\) (no scores seen yet).\n",
        "\n",
        "These live in HBM because they’re size \\(O(Nd)\\) / \\(O(N)\\), which is manageable.\n",
        "\n",
        "---\n",
        "\n",
        "### **3: Divide \\(Q\\) into \\(T_r=\\lceil N/B_r\\rceil\\) blocks** \\(Q_1,\\dots,Q_{T_r}\\) of size \\(B_r\\times d\\).  \n",
        "Divide \\(K,V\\) into \\(T_c=\\lceil N/B_c\\rceil\\) blocks \\(K_1,\\dots,K_{T_c}\\), \\(V_1,\\dots,V_{T_c}\\) of size \\(B_c\\times d\\).\n",
        "\n",
        "This is the tiling setup:\n",
        "\n",
        "- Row tiles: chunks of queries.\n",
        "- Column tiles: chunks of keys/values.\n",
        "\n",
        "So instead of forming the full \\(QK^T\\), you’ll compute it tile-by-tile: each \\((i,j)\\) tile is \\(B_r\\times B_c\\).\n",
        "\n",
        "---\n",
        "\n",
        "### **4: Divide \\(O,\\ell,m\\) into \\(T_r\\) blocks** \\(O_i, \\ell_i, m_i\\) matching the query blocks\n",
        "\n",
        "So when you work on \\(Q_i\\), you also work on the corresponding slice of:\n",
        "\n",
        "- output \\(O_i\\) (shape \\(B_r\\times d\\)),\n",
        "- running \\(\\ell_i\\) (shape \\(B_r\\)),\n",
        "- running \\(m_i\\) (shape \\(B_r\\)).\n",
        "\n",
        "---\n",
        "\n",
        "### **5: for \\(1\\le j \\le T_c\\) do**\n",
        "\n",
        "Outer loop over key/value tiles.\n",
        "\n",
        "Interpretation: “Fix a block of keys/values and reuse it across many query blocks.”\n",
        "\n",
        "---\n",
        "\n",
        "### **6: Load \\(K_j, V_j\\) from HBM to on-chip SRAM**\n",
        "\n",
        "Bring the current K/V tile into fast memory once.\n",
        "\n",
        "This is one of the big wins: you load \\(K_j, V_j\\) and then use them for *all* \\(Q_i\\) blocks (inside the next loop), rather than constantly spilling intermediates to HBM.\n",
        "\n",
        "---\n",
        "\n",
        "### **7: for \\(1\\le i \\le T_r\\) do**\n",
        "\n",
        "Inner loop over query tiles.\n",
        "\n",
        "So for each \\(K_j,V_j\\) tile, you sweep over all query blocks \\(Q_i\\).\n",
        "\n",
        "---\n",
        "\n",
        "### **8: Load \\(Q_i, O_i, \\ell_i, m_i\\) from HBM to on-chip SRAM**\n",
        "\n",
        "You need:\n",
        "\n",
        "- \\(Q_i\\) to compute scores against \\(K_j\\),\n",
        "- \\(O_i,\\ell_i,m_i\\) because FlashAttention updates the output **incrementally** as it processes each \\(K/V\\) tile.\n",
        "\n",
        "---\n",
        "\n",
        "### **9: On chip, compute** \\(S_{ij} = Q_i K_j^T \\in \\mathbb{R}^{B_r \\times B_c}\\)\n",
        "\n",
        "This is the attention score tile for the current query block vs key block.\n",
        "\n",
        "In standard attention, you’d build all of \\(S\\in\\mathbb{R}^{N\\times N}\\). Here you only form a tile \\(S_{ij}\\) on chip.\n",
        "\n",
        "*(Also: the usual scaling \\(1/\\sqrt{d}\\) can be applied by scaling \\(Q\\) beforehand or scaling \\(S_{ij}\\) here. The pseudocode often omits it to keep notation clean.)*\n",
        "\n",
        "---\n",
        "\n",
        "### **10: On chip, compute**\n",
        "\n",
        "- \\(\\tilde{m}_{ij} = \\text{rowmax}(S_{ij}) \\in \\mathbb{R}^{B_r}\\)\n",
        "- \\(\\tilde{P}_{ij} = \\exp(S_{ij} - \\tilde{m}_{ij}) \\in \\mathbb{R}^{B_r\\times B_c}\\) (rowwise subtract max, pointwise exp)\n",
        "- \\(\\tilde{\\ell}_{ij} = \\text{rowsum}(\\tilde{P}_{ij}) \\in \\mathbb{R}^{B_r}\\)\n",
        "\n",
        "This is the stable softmax prep **for just this tile**:\n",
        "\n",
        "- Subtracting \\(\\tilde{m}_{ij}\\) prevents overflow.\n",
        "- \\(\\tilde{\\ell}_{ij}\\) is the denominator *for this tile alone* (not the whole row across all keys).\n",
        "\n",
        "---\n",
        "\n",
        "### **11: On chip, update the running max and running normalizer**\n",
        "\n",
        "- \\(m_i^{new} = \\max(m_i, \\tilde{m}_{ij}) \\in \\mathbb{R}^{B_r}\\)\n",
        "- \\(\\ell_i^{new} = e^{m_i - m_i^{new}}\\ell_i \\;+\\; e^{\\tilde{m}_{ij}-m_i^{new}}\\tilde{\\ell}_{ij} \\in \\mathbb{R}^{B_r}\\)\n",
        "\n",
        "This is the “online softmax” heart of FlashAttention.\n",
        "\n",
        "Think per query row \\(r\\):\n",
        "\n",
        "- previously you had processed some key tiles, and stored:\n",
        "  - \\(m\\) = max score seen so far,\n",
        "  - \\(\\ell\\) = sum of exp(score - \\(m\\)) over keys seen so far.\n",
        "- now you process a new tile, which has its own max \\(\\tilde m\\) and sum \\(\\tilde \\ell\\).\n",
        "\n",
        "To combine them safely, you:\n",
        "\n",
        "1. update the max to \\(m^{new}\\),\n",
        "2. rescale old and new sums into the same reference max \\(m^{new}\\), then add.\n",
        "\n",
        "This exactly matches log-sum-exp stabilization, just done incrementally.\n",
        "\n",
        "---\n",
        "\n",
        "### **12: Write updated output**\n",
        "\n",
        "\\[\n",
        "O_i \\leftarrow \\text{diag}(\\ell_i^{new})^{-1}\n",
        "\\Big(\n",
        "\\text{diag}(\\ell_i)\\,e^{m_i-m_i^{new}}\\,O_i\n",
        "\\;+\\;\n",
        "e^{\\tilde{m}_{ij}-m_i^{new}} \\,\\tilde{P}_{ij} V_j\n",
        "\\Big)\n",
        "\\quad \\text{to HBM.}\n",
        "\\]\n",
        "\n",
        "What’s happening:\n",
        "\n",
        "- \\(\\tilde{P}_{ij}V_j\\) is the **numerator contribution** from the current tile (like “probabilities times values”, but unnormalized across *all* keys because we’re still streaming).\n",
        "- The old \\(O_i\\) you stored is already normalized using the *old* \\(\\ell_i\\) and max \\(m_i\\).  \n",
        "  So to “merge” it with the new tile, you first convert it back into a compatible numerator-like form:\n",
        "  - multiply by \\(\\ell_i\\),\n",
        "  - rescale by \\(e^{m_i-m_i^{new}}\\) because the reference max changed.\n",
        "- Add the new tile’s numerator (also rescaled to the same max reference).\n",
        "- Finally divide by the new denominator \\(\\ell_i^{new}\\) to get the updated normalized output.\n",
        "\n",
        "So line 12 is an **online update of the attention output** without ever storing the full attention matrix.\n",
        "\n",
        "---\n",
        "\n",
        "### **13: Write** \\(\\ell_i \\leftarrow \\ell_i^{new}\\), \\(m_i \\leftarrow m_i^{new}\\) to HBM\n",
        "\n",
        "Persist the updated running stats for this query block, so when you process the next \\(K/V\\) tile \\(j+1\\), you continue from the correct state.\n",
        "\n",
        "---\n",
        "\n",
        "### **14–15: end for loops**\n",
        "\n",
        "Finish all query blocks for this K/V tile, then move to the next K/V tile.\n",
        "\n",
        "At the very end, every query row has streamed over all keys, and its \\((m,\\ell,O)\\) represent the full softmax over all \\(N\\) keys.\n",
        "\n",
        "---\n",
        "\n",
        "### **16: Return \\(O\\)**\n",
        "\n",
        "Now \\(O\\) is the attention output.\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can also rewrite the core (lines 10–12) in a compact “per-row math” view so it’s easier to see that it exactly equals \\(\\text{softmax}(QK^T)V\\).\n"
      ],
      "metadata": {
        "id": "NG4X7yKnfIg7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LEY9jrBQfJ7g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}