{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-o2j6w9Qdw7B"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_naive(X: torch.Tensor, W_q, W_k, W_v,\n",
        "                    b_q, b_k, b_v, device) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "        X :     NxD (N: number of tokens, D: Dimension of latent space tokenizer)\n",
        "        W_*:    D_headxD (D_head: Model space dimension / num heads)\n",
        "    \"\"\"\n",
        "\n",
        "    # check if X is NxD\n",
        "    assert(X.dim()==2)\n",
        "\n",
        "    Q = torch.matmul(X, W_q.transpose(0,1)) + b_q[None, :]\n",
        "    K = torch.matmul(X, W_k.transpose(0,1)) + b_k[None, :]\n",
        "    V = torch.matmul(X, W_v.transpose(0,1)) + b_v[None, :]\n",
        "    D_V = torch.tensor(V.shape[1], device=device)\n",
        "\n",
        "    KQ_normalised = torch.matmul(Q, K.transpose(0,1)) / torch.sqrt(D_V)\n",
        "    KQ_softmax = torch.softmax(KQ_normalised, dim=1)\n",
        "\n",
        "    attention = torch.matmul(KQ_softmax, V)\n",
        "\n",
        "    return attention\n",
        "\n",
        "def multiheaded_attention_naive(X: torch.Tensor, W_qkv, W_out,\n",
        "                    b_qkv, b_out, num_heads=1, device=\"cuda\") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    W_qkv: 3DxD\n",
        "    W_out: DxD\n",
        "    b_qkv: 3D\n",
        "    b_out: D\n",
        "    \"\"\"\n",
        "    # check if X is NxD\n",
        "    assert(X.dim()==2)\n",
        "\n",
        "    N, D = X.shape\n",
        "    D_head = math.ceil(D / num_heads)\n",
        "    attention = torch.empty((N, D), device=device, dtype=torch.float16)\n",
        "\n",
        "    for head in range(num_heads):\n",
        "        head_start = head*D_head\n",
        "        head_end = min(D, (head+1)*D_head)\n",
        "        attention[:,head_start:head_end] = attention_naive(\n",
        "            X,\n",
        "            W_qkv[0:D, :][head_start:head_end, :],\n",
        "            W_qkv[D:2*D, :][head_start:head_end, :],\n",
        "            W_qkv[2*D:3*D, :][head_start:head_end, :],\n",
        "            b_qkv[0+head_start:0+head_end],\n",
        "            b_qkv[D+head_start:D+head_end],\n",
        "            b_qkv[2*D+head_start:2*D+head_end],\n",
        "            device\n",
        "        )\n",
        "\n",
        "    attention = torch.matmul(attention, W_out.transpose(0,1)) + b_out[None, :]\n",
        "\n",
        "    return attention"
      ],
      "metadata": {
        "id": "8At_PzfpeV0X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Timing helpers (your style)\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def time_ms0(fn, iters=100, warmup=25):\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end   = torch.cuda.Event(enable_timing=True)\n",
        "    start.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return start.elapsed_time(end) / iters\n",
        "\n",
        "def mha_naive_wrapper(X, mha: torch.nn.MultiheadAttention):\n",
        "\n",
        "    num_heads = mha.num_heads\n",
        "    in_proj_w = mha.in_proj_weight\n",
        "    in_proj_b = mha.in_proj_bias\n",
        "    out_proj = mha.out_proj\n",
        "    device = in_proj_w.device\n",
        "\n",
        "    attention_naive_out = multiheaded_attention_naive(\n",
        "        X,\n",
        "        in_proj_w,\n",
        "        out_proj.weight,\n",
        "        in_proj_b,\n",
        "        out_proj.bias,\n",
        "        num_heads,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    return attention_naive_out\n",
        "\n",
        "def mha_torch_wrapper(X, mha: torch.nn.MultiheadAttention):\n",
        "    # For (N, D) input, PyTorch interprets as (L, E) when unbatched.\n",
        "    # This returns (L, E). Use need_weights=False to time only output.\n",
        "    out, _ = mha(X, X, X, need_weights=False)\n",
        "    return out\n",
        "\n",
        "def report(name, ms, N, D):\n",
        "    toks_per_s = N / (ms / 1e3)\n",
        "    print(f\"{name:>14}: {ms:8.3f} ms | {toks_per_s:10.1f} tokens/s\")\n"
      ],
      "metadata": {
        "id": "xtmfHOIGeVt8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Run benchmark\n",
        "# -----------------------------\n",
        "torch.manual_seed(42)\n",
        "device=\"cuda\"\n",
        "N, D, H = 8192, 128, 2\n",
        "\n",
        "X = torch.randn((N, D), device=device, dtype=torch.float16)\n",
        "\n",
        "mha = torch.nn.MultiheadAttention(embed_dim=D, num_heads=H, device=device, dtype=torch.float16)\n",
        "mha.eval()\n",
        "\n",
        "# correctness check first\n",
        "with torch.no_grad():\n",
        "    ref = mha_torch_wrapper(X, mha)\n",
        "    out = mha_naive_wrapper(X, mha)\n",
        "    print(\"max abs err:\", (out - ref).abs().max().item())\n",
        "    print(\"mean abs err:\", (out - ref).abs().mean().item())\n",
        "\n",
        "# timing\n",
        "torch_ms = time_ms0(lambda: mha_torch_wrapper(X, mha), iters=100, warmup=25)\n",
        "naive_ms = time_ms0(lambda: mha_naive_wrapper(X, mha), iters=20, warmup=5)  # naive is O(N^2); use fewer iters\n",
        "\n",
        "report(\"torch_mha\", torch_ms, N, D)\n",
        "report(\"naive_mha\", naive_ms, N, D)"
      ],
      "metadata": {
        "id": "5STli3JOeVrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93d0c6d6-2fcf-4325-d145-0159084a9be7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max abs err: 6.103515625e-05\n",
            "mean abs err: 4.649162292480469e-06\n",
            "     torch_mha:    2.959 ms |  2768683.4 tokens/s\n",
            "     naive_mha:    9.990 ms |   820018.2 tokens/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U2nFVjYm6yEL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g_unOZXJzl-R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}