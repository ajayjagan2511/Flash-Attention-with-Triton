{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade triton"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTZyQYJMvIfP",
        "outputId": "d8018e1a-8785-437d-cb09-7452fb21c9cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (3.5.0)\n",
            "Collecting triton\n",
            "  Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.9.0+cu128 requires triton==3.5.0; platform_system == \"Linux\", but you have triton 3.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed triton-3.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-o2j6w9Qdw7B"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Triton version:\", triton.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUL49DvtupXC",
        "outputId": "9eed5272-f8b9-48d0-a50a-1a1af7555d27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triton version: 3.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_naive(X: torch.Tensor, W_q, W_k, W_v,\n",
        "                    b_q, b_k, b_v, device) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "        X :     NxD (N: number of tokens, D: Dimension of latent space tokenizer)\n",
        "        W_*:    D_headxD (D_head: Model space dimension / num heads)\n",
        "    \"\"\"\n",
        "\n",
        "    # check if X is NxD\n",
        "    assert(X.dim()==2)\n",
        "\n",
        "    Q = torch.matmul(X, W_q.transpose(0,1)) + b_q[None, :]\n",
        "    K = torch.matmul(X, W_k.transpose(0,1)) + b_k[None, :]\n",
        "    V = torch.matmul(X, W_v.transpose(0,1)) + b_v[None, :]\n",
        "    D_V = torch.tensor(V.shape[1], device=device)\n",
        "\n",
        "    KQ_normalised = torch.matmul(Q, K.transpose(0,1)) / torch.sqrt(D_V)\n",
        "    KQ_softmax = torch.softmax(KQ_normalised, dim=1)\n",
        "\n",
        "    attention = torch.matmul(KQ_softmax, V)\n",
        "\n",
        "    return attention\n",
        "\n",
        "# @torch.compile(fullgraph=True)\n",
        "def multiheaded_attention_naive(X: torch.Tensor, W_qkv, W_out,\n",
        "                    b_qkv, b_out, num_heads=1, device=\"cuda\") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    W_qkv: 3DxD\n",
        "    W_out: DxD\n",
        "    b_qkv: 3D\n",
        "    b_out: D\n",
        "    \"\"\"\n",
        "    # check if X is NxD\n",
        "    assert(X.dim()==2)\n",
        "\n",
        "    N, D = X.shape\n",
        "    D_head = math.ceil(D / num_heads)\n",
        "    attention = torch.empty((N, D), device=device, dtype=torch.float16)\n",
        "\n",
        "    for head in range(num_heads):\n",
        "        head_start = head*D_head\n",
        "        head_end = min(D, (head+1)*D_head)\n",
        "        attention[:,head_start:head_end] = attention_naive(\n",
        "            X,\n",
        "            W_qkv[0:D, :][head_start:head_end, :],\n",
        "            W_qkv[D:2*D, :][head_start:head_end, :],\n",
        "            W_qkv[2*D:3*D, :][head_start:head_end, :],\n",
        "            b_qkv[0+head_start:0+head_end],\n",
        "            b_qkv[D+head_start:D+head_end],\n",
        "            b_qkv[2*D+head_start:2*D+head_end],\n",
        "            device\n",
        "        )\n",
        "\n",
        "    attention = torch.matmul(attention, W_out.transpose(0,1)) + b_out[None, :]\n",
        "\n",
        "    return attention"
      ],
      "metadata": {
        "id": "8At_PzfpeV0X"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _attention(\n",
        "        Q, K, V,\n",
        "        O, l, m,\n",
        "        BLOCK_R : tl.constexpr,\n",
        "        BLOCK_C : tl.constexpr,\n",
        "        BLOCK_D : tl.constexpr,\n",
        "        N_q : tl.constexpr, N_v : tl.constexpr,\n",
        "        D_head : tl.constexpr, B_c : tl.constexpr, B_r : tl.constexpr,\n",
        "        T_r : tl.constexpr, T_c : tl.constexpr,\n",
        "        ):\n",
        "\n",
        "    head_idx = tl.program_id(0)\n",
        "    # transpose everything when passing args, so that moving across is linear.\n",
        "    Q_ptr = Q + (head_idx * N_q * D_head)\n",
        "    K_ptr = K + (head_idx * N_v * D_head)\n",
        "    V_ptr = V + (head_idx * N_v * D_head)\n",
        "    O_ptr = O + (head_idx * N_q * D_head)\n",
        "    l_ptr = l + (head_idx * N_q)\n",
        "    m_ptr = m + (head_idx * N_q)\n",
        "\n",
        "    # Get ptrs for loading K,V,Q,O into SRAM\n",
        "    D_indices = tl.arange(0, BLOCK_D)[:, None]          # (D_head, 1)\n",
        "    C_indices = tl.arange(0, BLOCK_C)[None, :]          # (1, B_c)\n",
        "    R_indices = tl.arange(0, BLOCK_R)[None, :]          # (1, B_r)\n",
        "    # Moving the start of each row by N_v steps\n",
        "    # then get B_c size slices from each row\n",
        "    K_row_start_ptrs = K_ptr + (D_indices * N_v)            # (D_head, 1)\n",
        "    K_j_ptrs = K_row_start_ptrs + C_indices                 # (D_head, B_c)\n",
        "    V_row_start_ptrs = V_ptr + (D_indices * N_v)            # (D_head, 1)\n",
        "    V_j_ptrs = V_row_start_ptrs + C_indices                 # (D_head, B_c)\n",
        "    mask_KV = (D_indices < D_head) & (C_indices < B_c)      # (BLOCK_D, BLOCK_C)\n",
        "\n",
        "    # Same for Q and O\n",
        "    Q_row_start_ptrs = Q_ptr + (D_indices * N_q)            # (D_head, 1)\n",
        "    Q_i_ptrs = Q_row_start_ptrs + R_indices                 # (D_head, B_r)\n",
        "    O_row_start_ptrs = O_ptr + (D_indices * N_q)            # (D_head, 1)\n",
        "    O_i_ptrs = O_row_start_ptrs + R_indices                 # (D_head, B_r)\n",
        "    mask_QO = (D_indices < D_head) & (R_indices < B_r)      # (BLOCK_D, BLOCK_R)\n",
        "\n",
        "    # Get the pointers for l and m\n",
        "    l_i_ptrs = l_ptr + R_indices                            # (BLOCK_R,)\n",
        "    m_i_ptrs = m_ptr + R_indices                            # (BLOCK_R,)\n",
        "    mask_lm = R_indices < B_r                               # (BLOCK_R,)\n",
        "\n",
        "    for j in range(T_c):\n",
        "        K_j = tl.load(K_j_ptrs, mask=mask_KV, other=0.0)    # (D_head, B_c)\n",
        "        V_j = tl.load(V_j_ptrs, mask=mask_KV, other=0.0).to(tl.float32)    # (D_head, B_c)\n",
        "\n",
        "        for i in range(T_r):\n",
        "            Q_i = tl.load(Q_i_ptrs, mask=mask_QO, other=0.0)                    # (D_head, B_r)\n",
        "            O_i = tl.load(O_i_ptrs, mask=mask_QO, other=0.0).to(tl.float32)     # (D_head, B_r)\n",
        "            l_i = tl.load(l_i_ptrs, mask=mask_lm, other=0.0).to(tl.float32)\n",
        "            m_i = tl.load(m_i_ptrs, mask=mask_lm, other=float('-inf')).to(tl.float32)\n",
        "\n",
        "            # Compute S_ij = Q_i x K_j^T\n",
        "            scale = 1.0 / tl.sqrt(tl.full([], D_head, dtype=tl.float32))  # scalar\n",
        "            S_ij = tl.dot(K_j.T, Q_i).to(tl.float32) * scale               # (B_c, B_r)\n",
        "\n",
        "            # get the rowmax for S_ij\n",
        "            m_ij = tl.max(S_ij, axis=0)                     # (B_r,)\n",
        "\n",
        "            # compute exponents for softmax after subtracting max\n",
        "            # element for numerical stability\n",
        "            P_ij = tl.exp(S_ij - m_ij)                      # (B_c, B_r)\n",
        "\n",
        "            # Compute the row sums for softmax\n",
        "            l_ij = tl.sum(P_ij, axis=0)                     # (B_r,)\n",
        "\n",
        "            # Update the global max m_i till this point\n",
        "            # tl.cat threw an error when given dim=0, weird but okay because it defaults to 0\n",
        "            # m_i_new = tl.max(tl.cat(m_ij, m_i[None, :]), axis=0, keep_dims=True)     # (1, B_r)\n",
        "            m_i_new = tl.maximum(m_ij, m_i)                 # (B_r,)\n",
        "            alpha = tl.exp(m_i - m_i_new)                   # (B_r,)\n",
        "            beta  = tl.exp(m_ij - m_i_new)                  # (B_r,)\n",
        "            l_i_new = alpha * l_i + beta * l_ij               # (B_r,)\n",
        "\n",
        "            # Update the output\n",
        "            # create a diagonal matrix with l_i on diag\n",
        "            # mask_primitive = tl.arange(0, l_i.shape[0])\n",
        "            # diag_l_i = l_i[None, :] * (mask_primitive[:, None] == mask_primitive[None, :])\n",
        "            # diag_l_i_new_inv = (1 / l_i_new) * (mask_primitive[:, None] == mask_primitive[None, :])\n",
        "\n",
        "            # Broadcasting works here as the last dim is matched (D_head, Br) and (Br)\n",
        "            PV = tl.dot(V_j, P_ij)                          # (D_head, B_r)\n",
        "            O_i = ((l_i * alpha) * O_i + beta * PV) / l_i_new       # (D_head, B_r)\n",
        "\n",
        "            # Store the computed O_i, l_i and m_i values\n",
        "            tl.store(O_i_ptrs, O_i, mask=mask_QO)\n",
        "            tl.store(l_i_ptrs, l_i_new, mask=mask_lm)\n",
        "            tl.store(m_i_ptrs, m_i_new, mask=mask_lm)\n",
        "\n",
        "            Q_i_ptrs += B_r\n",
        "            O_i_ptrs += B_r\n",
        "            l_i_ptrs += B_r\n",
        "            m_i_ptrs += B_r\n",
        "\n",
        "        K_j_ptrs += B_c    # shift ptrs for next iter\n",
        "        V_j_ptrs += B_c\n",
        "        Q_i_ptrs -= N_q\n",
        "        O_i_ptrs -= N_q\n",
        "        l_i_ptrs -= N_q\n",
        "        m_i_ptrs -= N_q\n",
        "\n",
        "\n",
        "def multiheaded_attention_triton(\n",
        "        query: torch.Tensor,\n",
        "        key: torch.Tensor,\n",
        "        value: torch.Tensor,\n",
        "        W_qkv, W_out,\n",
        "        b_qkv, b_out,\n",
        "        num_heads=1,\n",
        "        device=\"cuda\") -> torch.Tensor:\n",
        "\n",
        "    N_q, D = query.shape\n",
        "    N_v, _ = value.shape\n",
        "    dtype = W_qkv.dtype\n",
        "\n",
        "    M = 16*1024     # HARD CODED SRAM\n",
        "\n",
        "    # Q = torch.matmul(query, W_qkv[0:D, :].transpose(0,1)) + b_qkv[0:D][None, :]\n",
        "    # K = torch.matmul(query, W_qkv[D:2*D, :].transpose(0,1)) + b_qkv[D:2*D][None, :]\n",
        "    # V = torch.matmul(query, W_qkv[2*D:3*D, :].transpose(0,1)) + b_qkv[2*D:3*D][None, :]\n",
        "\n",
        "    # buffers\n",
        "    Q = torch.zeros((N_q, D), device=device, dtype=dtype)\n",
        "    K = torch.zeros((N_v, D), device=device, dtype=dtype)\n",
        "    V = torch.zeros((N_v, D), device=device, dtype=dtype)\n",
        "    D_head = math.ceil(D / num_heads)\n",
        "\n",
        "    for head in range(num_heads):\n",
        "        hs = head * D_head\n",
        "        he = min(D, (head + 1) * D_head)\n",
        "\n",
        "        Wq = W_qkv[0:D, :][hs:he, :]\n",
        "        Wk = W_qkv[D:2*D, :][hs:he, :]\n",
        "        Wv = W_qkv[2*D:3*D, :][hs:he, :]\n",
        "\n",
        "        bq = b_qkv[hs:he]\n",
        "        bk = b_qkv[D + hs:D + he]\n",
        "        bv = b_qkv[2*D + hs:2*D + he]\n",
        "\n",
        "        Q[:, hs:he] = (query @ Wq.T) + bq[None, :]\n",
        "        K[:, hs:he] = (key   @ Wk.T) + bk[None, :]\n",
        "        V[:, hs:he] = (value @ Wv.T) + bv[None, :]\n",
        "\n",
        "    # multiheads are cascaded column wise, by taking the transpose, we can access each head in continuous space\n",
        "    Q = Q.T.contiguous()\n",
        "    K = K.T.contiguous()\n",
        "    V = V.T.contiguous()\n",
        "\n",
        "    B_c = math.ceil(M / (4*D))\n",
        "    B_r = min(math.ceil(M / (4*D)), D)\n",
        "\n",
        "    O = torch.zeros((D, N_q), device=device, dtype=torch.float32)    # transposed to match Q,K,V\n",
        "\n",
        "    l = torch.zeros((num_heads, N_q), device=device, dtype=torch.float32)\n",
        "    m = torch.full((num_heads, N_q), float('-inf'), device=device, dtype=torch.float32)\n",
        "\n",
        "    T_r = math.ceil(N_q / B_r)\n",
        "    T_c = math.ceil(N_v / B_c)\n",
        "\n",
        "    BLOCK_R = triton.next_power_of_2(B_r)\n",
        "    BLOCK_C = triton.next_power_of_2(B_c)\n",
        "    BLOCK_D = triton.next_power_of_2(D_head)\n",
        "\n",
        "    num_warps = 8\n",
        "    grid = (num_heads, )\n",
        "\n",
        "    _attention[grid](\n",
        "        Q, K, V,\n",
        "        O, l, m,\n",
        "        BLOCK_R, BLOCK_C, BLOCK_D,\n",
        "        N_q, N_v,\n",
        "        D_head, B_c, B_r,\n",
        "        T_r, T_c,\n",
        "        num_warps=num_warps\n",
        "    )\n",
        "\n",
        "    #print(O.dtype, W_out.dtype)\n",
        "\n",
        "    O = O.to(torch.float16)\n",
        "    attention = torch.matmul(O.T, W_out.transpose(0,1)) + b_out[None, :]\n",
        "\n",
        "    return attention"
      ],
      "metadata": {
        "id": "pPaq8E4GZYHr"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Timing helpers (your style)\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def time_ms0(fn, iters=100, warmup=25):\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end   = torch.cuda.Event(enable_timing=True)\n",
        "    start.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return start.elapsed_time(end) / iters\n",
        "\n",
        "def mha_naive_wrapper(X, mha: torch.nn.MultiheadAttention):\n",
        "    num_heads = mha.num_heads\n",
        "    in_proj_w = mha.in_proj_weight\n",
        "    in_proj_b = mha.in_proj_bias\n",
        "    out_proj = mha.out_proj\n",
        "    device = in_proj_w.device\n",
        "\n",
        "    attention_naive_out = multiheaded_attention_naive(\n",
        "        X,\n",
        "        in_proj_w,\n",
        "        out_proj.weight,\n",
        "        in_proj_b,\n",
        "        out_proj.bias,\n",
        "        num_heads,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    return attention_naive_out\n",
        "\n",
        "def mha_triton_wrapper(X, mha: torch.nn.MultiheadAttention):\n",
        "    num_heads = mha.num_heads\n",
        "    in_proj_w = mha.in_proj_weight\n",
        "    in_proj_b = mha.in_proj_bias\n",
        "    out_proj = mha.out_proj\n",
        "    device = in_proj_w.device\n",
        "\n",
        "    attention_triton_out = multiheaded_attention_triton(\n",
        "        X, X, X,\n",
        "        in_proj_w,\n",
        "        out_proj.weight,\n",
        "        in_proj_b,\n",
        "        out_proj.bias,\n",
        "        num_heads,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    return attention_triton_out\n",
        "\n",
        "def mha_torch_wrapper(X, mha: torch.nn.MultiheadAttention):\n",
        "    # For (N, D) input, PyTorch interprets as (L, E) when unbatched.\n",
        "    # This returns (L, E). Use need_weights=False to time only output.\n",
        "    out, _ = mha(X, X, X, need_weights=False)\n",
        "    return out\n",
        "\n",
        "def report(name, ms, N, D):\n",
        "    toks_per_s = N / (ms / 1e3)\n",
        "    print(f\"{name:>14}: {ms:8.3f} ms | {toks_per_s:10.1f} tokens/s\")\n"
      ],
      "metadata": {
        "id": "xtmfHOIGeVt8"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Run benchmark\n",
        "# -----------------------------\n",
        "torch.manual_seed(42)\n",
        "device=\"cuda\"\n",
        "N, D, H = 1024, 128, 2\n",
        "\n",
        "X = torch.randn((N, D), device=device, dtype=torch.float16)\n",
        "\n",
        "mha = torch.nn.MultiheadAttention(embed_dim=D, num_heads=H, device=device, dtype=torch.float16)\n",
        "mha.eval()\n",
        "\n",
        "# correctness check first\n",
        "with torch.no_grad():\n",
        "    ref = mha_torch_wrapper(X, mha)\n",
        "    out = mha_naive_wrapper(X, mha)\n",
        "    out_triton = mha_triton_wrapper(X, mha)\n",
        "    print(\"max abs err:\", (out - ref).abs().max().item())\n",
        "    print(\"mean abs err:\", (out - ref).abs().mean().item())\n",
        "    print(\"triton max abs err:\", (out_triton - ref).abs().max().item())\n",
        "    print(\"triton mean abs err:\", (out_triton - ref).abs().mean().item())\n",
        "    print(\"No. of zeros:\", (out_triton == 0).sum().item())\n",
        "\n",
        "# timing\n",
        "torch_ms = time_ms0(lambda: mha_torch_wrapper(X, mha), iters=100, warmup=25)\n",
        "naive_ms = time_ms0(lambda: mha_naive_wrapper(X, mha), iters=20, warmup=5)  # naive is O(N^2); use fewer iters\n",
        "triton_ms = time_ms0(lambda: mha_triton_wrapper(X, mha), iters=100, warmup=25)\n",
        "\n",
        "report(\"torch_mha\", torch_ms, N, D)\n",
        "report(\"naive_mha\", naive_ms, N, D)\n",
        "report(\"triton_mha\", triton_ms, N, D)"
      ],
      "metadata": {
        "id": "5STli3JOeVrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42337af8-2ffc-495e-b23c-79143b353067"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max abs err: 6.103515625e-05\n",
            "mean abs err: 5.781650543212891e-06\n",
            "triton max abs err: 6.103515625e-05\n",
            "triton mean abs err: 8.046627044677734e-06\n",
            "No. of zeros: 0\n",
            "     torch_mha:    0.257 ms |  3986923.0 tokens/s\n",
            "     naive_mha:    0.854 ms |  1198609.6 tokens/s\n",
            "    triton_mha:    3.739 ms |   273854.8 tokens/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.full((5,), float('inf'), device=device)"
      ],
      "metadata": {
        "id": "U2nFVjYm6yEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "id": "g_unOZXJzl-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "925d3ce2-5253-4df5-d12c-97262ce0f358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([inf, inf, inf, inf, inf], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.zeros((1,5))\n",
        "x.shape, x[0,:].shape"
      ],
      "metadata": {
        "id": "5CA5TFbBi6kC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "627e1ced-6da9-47d0-fe55-6d2ce2c76301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5]), torch.Size([5]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What FlashAttention improves (why it’s faster + more memory-friendly)\n",
        "\n",
        "Vanilla attention does this (per head):\n",
        "\n",
        "\\[\n",
        "O = \\text{softmax}(QK^T)\\,V\n",
        "\\]\n",
        "\n",
        "The expensive part isn’t the math—it’s **memory traffic**:\n",
        "\n",
        "- Computing \\(S = QK^T\\) makes an \\(N\\times N\\) score matrix.\n",
        "- Then softmax makes another \\(N\\times N\\) matrix \\(P\\).\n",
        "- Writing/reading those huge matrices to **HBM (GPU high-bandwidth memory)** dominates runtime and blows up memory.\n",
        "\n",
        "**FlashAttention’s key idea**: never materialize \\(S\\) or \\(P\\) in HBM.  \n",
        "Instead it:\n",
        "\n",
        "- **tiles** \\(Q\\) and \\(K,V\\) into blocks that fit in fast **on-chip SRAM** (shared memory/registers),\n",
        "- computes softmax **incrementally** using an **online, numerically stable** update (running max + running normalizer),\n",
        "- **fuses** operations so the main loop is “load a tile → compute → accumulate output” with minimal writes.\n",
        "\n",
        "**Benefits you get:**\n",
        "\n",
        "- **Much lower memory usage** (no \\(N^2\\) intermediates stored).\n",
        "- **Much higher speed** by reducing HBM reads/writes and increasing arithmetic intensity.\n",
        "- Still computes the **exact same attention** as standard (up to floating point rounding), not an approximation.\n",
        "- The online softmax update is **numerically stable** (like standard log-sum-exp stabilization).\n",
        "\n",
        "---\n",
        "\n",
        "## FlashAttention Algorithm 1 — line by line\n",
        "\n",
        "I’ll keep a running dictionary of symbols:\n",
        "\n",
        "- \\(Q,K,V \\in \\mathbb{R}^{N\\times d}\\): query/key/value (for a single head typically).\n",
        "- HBM = GPU global memory, big but slower.\n",
        "- SRAM = on-chip memory (shared/register), small but fast.\n",
        "- \\(M\\): SRAM capacity budget for the kernel.\n",
        "- \\(O \\in \\mathbb{R}^{N\\times d}\\): the attention output we want.\n",
        "- For softmax stability per row, we track:\n",
        "  - \\(m\\in\\mathbb{R}^N\\): running **row max** of scores seen so far\n",
        "  - \\(\\ell\\in\\mathbb{R}^N\\): running **row sum of \\(\\exp(\\text{scores} - \\max)\\)** (the softmax denominator)\n",
        "\n",
        "---\n",
        "\n",
        "### **Require:** Matrices \\(Q,K,V\\in\\mathbb{R}^{N\\times d}\\) in HBM, on-chip SRAM of size \\(M\\)\n",
        "\n",
        "You start with Q/K/V stored in global memory. You have a limited SRAM scratchpad to hold small tiles.\n",
        "\n",
        "---\n",
        "\n",
        "### **1: Set block sizes** \\(B_c = \\left\\lfloor\\frac{M}{4d}\\right\\rfloor\\), \\(B_r=\\min\\left(\\left\\lfloor\\frac{M}{4d}\\right\\rfloor,\\, d\\right)\\)\n",
        "\n",
        "Pick tile sizes so the working set fits in SRAM.\n",
        "\n",
        "- \\(B_c\\): how many **keys/values** rows you load at once (a “column tile”).\n",
        "- \\(B_r\\): how many **queries** rows you process at once (a “row tile”).\n",
        "\n",
        "The \\(\\frac{M}{4d}\\) heuristic comes from “SRAM must hold several \\((\\text{tile rows})\\times d\\) arrays at once” (e.g., Q tile, K tile, V tile, plus intermediates). The constant “4” is a budgeting factor.\n",
        "\n",
        "---\n",
        "\n",
        "### **2: Initialize** \\(O=(0)_{N\\times d}\\), \\(\\ell=(0)_N\\), \\(m=(-\\infty)_N\\) in HBM\n",
        "\n",
        "For each query row:\n",
        "\n",
        "- output starts at 0,\n",
        "- running denominator \\(\\ell\\) starts at 0 (no probability mass accumulated yet),\n",
        "- running max \\(m\\) starts at \\(-\\infty\\) (no scores seen yet).\n",
        "\n",
        "These live in HBM because they’re size \\(O(Nd)\\) / \\(O(N)\\), which is manageable.\n",
        "\n",
        "---\n",
        "\n",
        "### **3: Divide \\(Q\\) into \\(T_r=\\lceil N/B_r\\rceil\\) blocks** \\(Q_1,\\dots,Q_{T_r}\\) of size \\(B_r\\times d\\).  \n",
        "Divide \\(K,V\\) into \\(T_c=\\lceil N/B_c\\rceil\\) blocks \\(K_1,\\dots,K_{T_c}\\), \\(V_1,\\dots,V_{T_c}\\) of size \\(B_c\\times d\\).\n",
        "\n",
        "This is the tiling setup:\n",
        "\n",
        "- Row tiles: chunks of queries.\n",
        "- Column tiles: chunks of keys/values.\n",
        "\n",
        "So instead of forming the full \\(QK^T\\), you’ll compute it tile-by-tile: each \\((i,j)\\) tile is \\(B_r\\times B_c\\).\n",
        "\n",
        "---\n",
        "\n",
        "### **4: Divide \\(O,\\ell,m\\) into \\(T_r\\) blocks** \\(O_i, \\ell_i, m_i\\) matching the query blocks\n",
        "\n",
        "So when you work on \\(Q_i\\), you also work on the corresponding slice of:\n",
        "\n",
        "- output \\(O_i\\) (shape \\(B_r\\times d\\)),\n",
        "- running \\(\\ell_i\\) (shape \\(B_r\\)),\n",
        "- running \\(m_i\\) (shape \\(B_r\\)).\n",
        "\n",
        "---\n",
        "\n",
        "### **5: for \\(1\\le j \\le T_c\\) do**\n",
        "\n",
        "Outer loop over key/value tiles.\n",
        "\n",
        "Interpretation: “Fix a block of keys/values and reuse it across many query blocks.”\n",
        "\n",
        "---\n",
        "\n",
        "### **6: Load \\(K_j, V_j\\) from HBM to on-chip SRAM**\n",
        "\n",
        "Bring the current K/V tile into fast memory once.\n",
        "\n",
        "This is one of the big wins: you load \\(K_j, V_j\\) and then use them for *all* \\(Q_i\\) blocks (inside the next loop), rather than constantly spilling intermediates to HBM.\n",
        "\n",
        "---\n",
        "\n",
        "### **7: for \\(1\\le i \\le T_r\\) do**\n",
        "\n",
        "Inner loop over query tiles.\n",
        "\n",
        "So for each \\(K_j,V_j\\) tile, you sweep over all query blocks \\(Q_i\\).\n",
        "\n",
        "---\n",
        "\n",
        "### **8: Load \\(Q_i, O_i, \\ell_i, m_i\\) from HBM to on-chip SRAM**\n",
        "\n",
        "You need:\n",
        "\n",
        "- \\(Q_i\\) to compute scores against \\(K_j\\),\n",
        "- \\(O_i,\\ell_i,m_i\\) because FlashAttention updates the output **incrementally** as it processes each \\(K/V\\) tile.\n",
        "\n",
        "---\n",
        "\n",
        "### **9: On chip, compute** \\(S_{ij} = Q_i K_j^T \\in \\mathbb{R}^{B_r \\times B_c}\\)\n",
        "\n",
        "This is the attention score tile for the current query block vs key block.\n",
        "\n",
        "In standard attention, you’d build all of \\(S\\in\\mathbb{R}^{N\\times N}\\). Here you only form a tile \\(S_{ij}\\) on chip.\n",
        "\n",
        "*(Also: the usual scaling \\(1/\\sqrt{d}\\) can be applied by scaling \\(Q\\) beforehand or scaling \\(S_{ij}\\) here. The pseudocode often omits it to keep notation clean.)*\n",
        "\n",
        "---\n",
        "\n",
        "### **10: On chip, compute**\n",
        "\n",
        "- \\(\\tilde{m}_{ij} = \\text{rowmax}(S_{ij}) \\in \\mathbb{R}^{B_r}\\)\n",
        "- \\(\\tilde{P}_{ij} = \\exp(S_{ij} - \\tilde{m}_{ij}) \\in \\mathbb{R}^{B_r\\times B_c}\\) (rowwise subtract max, pointwise exp)\n",
        "- \\(\\tilde{\\ell}_{ij} = \\text{rowsum}(\\tilde{P}_{ij}) \\in \\mathbb{R}^{B_r}\\)\n",
        "\n",
        "This is the stable softmax prep **for just this tile**:\n",
        "\n",
        "- Subtracting \\(\\tilde{m}_{ij}\\) prevents overflow.\n",
        "- \\(\\tilde{\\ell}_{ij}\\) is the denominator *for this tile alone* (not the whole row across all keys).\n",
        "\n",
        "---\n",
        "\n",
        "### **11: On chip, update the running max and running normalizer**\n",
        "\n",
        "- \\(m_i^{new} = \\max(m_i, \\tilde{m}_{ij}) \\in \\mathbb{R}^{B_r}\\)\n",
        "- \\(\\ell_i^{new} = e^{m_i - m_i^{new}}\\ell_i \\;+\\; e^{\\tilde{m}_{ij}-m_i^{new}}\\tilde{\\ell}_{ij} \\in \\mathbb{R}^{B_r}\\)\n",
        "\n",
        "This is the “online softmax” heart of FlashAttention.\n",
        "\n",
        "Think per query row \\(r\\):\n",
        "\n",
        "- previously you had processed some key tiles, and stored:\n",
        "  - \\(m\\) = max score seen so far,\n",
        "  - \\(\\ell\\) = sum of exp(score - \\(m\\)) over keys seen so far.\n",
        "- now you process a new tile, which has its own max \\(\\tilde m\\) and sum \\(\\tilde \\ell\\).\n",
        "\n",
        "To combine them safely, you:\n",
        "\n",
        "1. update the max to \\(m^{new}\\),\n",
        "2. rescale old and new sums into the same reference max \\(m^{new}\\), then add.\n",
        "\n",
        "This exactly matches log-sum-exp stabilization, just done incrementally.\n",
        "\n",
        "---\n",
        "\n",
        "### **12: Write updated output**\n",
        "\n",
        "\\[\n",
        "O_i \\leftarrow \\text{diag}(\\ell_i^{new})^{-1}\n",
        "\\Big(\n",
        "\\text{diag}(\\ell_i)\\,e^{m_i-m_i^{new}}\\,O_i\n",
        "\\;+\\;\n",
        "e^{\\tilde{m}_{ij}-m_i^{new}} \\,\\tilde{P}_{ij} V_j\n",
        "\\Big)\n",
        "\\quad \\text{to HBM.}\n",
        "\\]\n",
        "\n",
        "What’s happening:\n",
        "\n",
        "- \\(\\tilde{P}_{ij}V_j\\) is the **numerator contribution** from the current tile (like “probabilities times values”, but unnormalized across *all* keys because we’re still streaming).\n",
        "- The old \\(O_i\\) you stored is already normalized using the *old* \\(\\ell_i\\) and max \\(m_i\\).  \n",
        "  So to “merge” it with the new tile, you first convert it back into a compatible numerator-like form:\n",
        "  - multiply by \\(\\ell_i\\),\n",
        "  - rescale by \\(e^{m_i-m_i^{new}}\\) because the reference max changed.\n",
        "- Add the new tile’s numerator (also rescaled to the same max reference).\n",
        "- Finally divide by the new denominator \\(\\ell_i^{new}\\) to get the updated normalized output.\n",
        "\n",
        "So line 12 is an **online update of the attention output** without ever storing the full attention matrix.\n",
        "\n",
        "---\n",
        "\n",
        "### **13: Write** \\(\\ell_i \\leftarrow \\ell_i^{new}\\), \\(m_i \\leftarrow m_i^{new}\\) to HBM\n",
        "\n",
        "Persist the updated running stats for this query block, so when you process the next \\(K/V\\) tile \\(j+1\\), you continue from the correct state.\n",
        "\n",
        "---\n",
        "\n",
        "### **14–15: end for loops**\n",
        "\n",
        "Finish all query blocks for this K/V tile, then move to the next K/V tile.\n",
        "\n",
        "At the very end, every query row has streamed over all keys, and its \\((m,\\ell,O)\\) represent the full softmax over all \\(N\\) keys.\n",
        "\n",
        "---\n",
        "\n",
        "### **16: Return \\(O\\)**\n",
        "\n",
        "Now \\(O\\) is the attention output.\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can also rewrite the core (lines 10–12) in a compact “per-row math” view so it’s easier to see that it exactly equals \\(\\text{softmax}(QK^T)V\\).\n"
      ],
      "metadata": {
        "id": "NG4X7yKnfIg7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LEY9jrBQfJ7g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}