{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl"
      ],
      "metadata": {
        "id": "_xr7RjobI020"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_naive_torch(x: torch.Tensor) -> torch.Tensor:\n",
        "    x_max = x.max(dim=1, keepdim=True).values          # (N, 1)\n",
        "    safe_x = x - x_max                                 # (N, M)\n",
        "    exp_x = torch.exp(safe_x)                          # (N, M)\n",
        "    denom = exp_x.sum(dim=1, keepdim=True)             # (N, 1)\n",
        "    return exp_x / denom                               # (N, M)"
      ],
      "metadata": {
        "id": "CiDGuqOmI01A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_pytorch(x: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "xnnKS-lfL8eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _softmax(Y, stride_y_row, X, stride_x_row, M, N, BLOCK_SIZE : tl.constexpr):\n",
        "    # program idx gives row idx\n",
        "    row_idx = tl.program_id(0)\n",
        "\n",
        "    input_row_ptr = X + (row_idx * stride_x_row)\n",
        "    col_indices = tl.arange(0,BLOCK_SIZE)\n",
        "    input_ptrs = input_row_ptr + col_indices\n",
        "\n",
        "    mask = col_indices < N      # mask for valid inputs\n",
        "\n",
        "    row = tl.load(input_ptrs, mask=mask, other=float(\"-inf\"))\n",
        "\n",
        "    row = row - tl.max(row, axis=0)\n",
        "    row = tl.exp(row)\n",
        "    denom = tl.sum(row, axis=0)\n",
        "    row = row / denom\n",
        "\n",
        "    output_row_ptr = Y + (row_idx * stride_y_row)\n",
        "    output_ptrs = output_row_ptr + col_indices\n",
        "\n",
        "    tl.store(output_ptrs, row, mask=mask)\n",
        "\n",
        "def softmax_triton(X: torch.Tensor) -> torch.Tensor:\n",
        "    # Allocate input/output tensors\n",
        "    rows, cols = X.shape\n",
        "    assert(X.dim()==2)\n",
        "\n",
        "    BLOCK_SIZE = triton.next_power_of_2(cols)\n",
        "\n",
        "    num_warms = 4\n",
        "    if BLOCK_SIZE > 2047:\n",
        "        num_warps = 8\n",
        "    if BLOCK_SIZE > 4095:\n",
        "        num_warps = 16\n",
        "\n",
        "    Y = torch.empty_like(X)     # output buffer\n",
        "\n",
        "    # SPMD launch grid\n",
        "    grid = (rows, )\n",
        "    # enqueue GPU kernel\n",
        "    _softmax[grid](Y, Y.stride(0),\n",
        "                X, X.stride(0),\n",
        "                rows, cols, BLOCK_SIZE,\n",
        "                num_warps=num_warps)\n",
        "\n",
        "    return Y"
      ],
      "metadata": {
        "id": "htNTGlx2P0ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Timing helpers\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def time_ms(fn, x, iters=100, warmup=25):\n",
        "    # Warmup\n",
        "    for _ in range(warmup):\n",
        "        fn(x)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end   = torch.cuda.Event(enable_timing=True)\n",
        "    start.record()\n",
        "    for _ in range(iters):\n",
        "        fn(x)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return start.elapsed_time(end) / iters"
      ],
      "metadata": {
        "id": "174Bw9RMKzop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "device = \"cuda\"\n",
        "\n",
        "# Choose shapes that make IO matter:\n",
        "# M large enough to keep SMs busy, N moderately large\n",
        "M = 81920\n",
        "N = 4096  # try 1024, 2048, 4096, 8192\n",
        "\n",
        "x = torch.randn((M, N), device=device, dtype=torch.float16)\n",
        "\n",
        "# Correctness check (compare to torch.softmax ONLY for validation, not timing)\n",
        "ref = softmax_pytorch(x)\n",
        "naive = softmax_naive_torch(x)\n",
        "max_err = (naive - ref).abs().max().item()\n",
        "print(f\"max abs error naive vs torch.softmax: {max_err:.3e}\")\n",
        "\n",
        "y_triton = softmax_triton(x)\n",
        "max_err = (y_triton - ref).abs().max().item()\n",
        "print(f\"max abs error triton vs torch.softmax: {max_err:.3e}\")\n",
        "\n",
        "# Benchmarks\n",
        "t_naive  = time_ms(softmax_naive_torch, x)\n",
        "t_pytorch = time_ms(softmax_pytorch, x)\n",
        "t_triton = time_ms(softmax_triton, x)\n",
        "\n",
        "print(f\"naive torch composition: {t_naive:.3f} ms\")\n",
        "print(f\"fused pytorch softmax:    {t_pytorch:.3f} ms\")\n",
        "print(f\"fused triton softmax:    {t_triton:.3f} ms\")\n",
        "print(f\"speedup pytorch native:                {t_naive / t_pytorch:.2f}x\")\n",
        "print(f\"speedup triton:                {t_naive / t_triton:.2f}x\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S6Xv5I3I0yZ",
        "outputId": "679a0a4b-0cfb-4b6a-ccb2-e8ae267e8a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max abs error naive vs torch.softmax: 3.052e-05\n",
            "max abs error triton vs torch.softmax: 3.815e-06\n",
            "naive torch composition: 4.587 ms\n",
            "fused pytorch softmax:    1.823 ms\n",
            "fused triton softmax:    0.773 ms\n",
            "speedup pytorch native:                2.52x\n",
            "speedup triton:                5.93x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uIJ7c4I0TiJM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}